{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "# headers needed to simulate agent\n",
    "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lade pandas csv set mit den heutigen ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lade pandas csv set mit den heutigen ids\n",
    "\n",
    "# von der Startseite aus befinden sich alle Artikel in <article> und dann a tag\n",
    "# nicht werbe artikel enthalten den zone teaser standard\n",
    "# zplus artikel enthalten data-zplus und normale artikel nicht\n",
    "\n",
    "# was will ich alles haben aus dem article? Author, Summary, Topic, Date, Quelle, Title, Text, Tags\n",
    "\n",
    "# Title und Topic befinden sich in einer <h1 class=\"article-heading\">\n",
    "# Title: befindet sich in einer <span class=\"article-heading__title\">\n",
    "# Topic: befindet sich in einer <span class=\"article-heading__kicker\">\n",
    "\n",
    "# Summary: befindet sich in einer <div class=\"summary\">\n",
    "# Author: befindet sich in einem link auf der Seite, welcher /autoren/RANDOM_BUCHSTABE/NAME_AUTHOR\n",
    "\n",
    "# Date und Quelle befinden sich in einer <div class=\"metadata\">\n",
    "# Date: befindet sich in einer <time class=\"metadata__date\"> in dem attribut datetime\n",
    "# Quelle befindet sich in einer <span class=\"metadata__source\">\n",
    "\n",
    "# Text: befindet sich Ã¼berall wo ein <p class=\"paragraph article__item\"> ist\n",
    "# Tags: befindet sich in einer <nav class=\"article-tags\"> in einer <ul class=\"article-tags__list\"> in allen <li><a> elements im Text \n",
    "\n",
    "\n",
    "\n",
    "# rufe alle normalen artikel auf, die auf der startseite gefunden werden konnten und noch nicht in dem File mit dem heutigen Zeug sind\n",
    "# die id eines artikels befindet sich in der data-uuid im body element des texts\n",
    "\n",
    "# speichere in datei mit folgenden namen: jahr-monat-tag-newspaper\n",
    "# speichere in datei mit folgenden namen: jahr-monat-tag-hardcoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the article links from the homepage or relevant section\n",
    "def get_article_links(base_url, num_articles):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the article links (you might need to adjust the selector based on the actual site structure)\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        url = link['href']\n",
    "        if url.startswith('/'):\n",
    "            url = base_url + url\n",
    "        if url.startswith(base_url) and 'article' in url:\n",
    "            links.append(url)\n",
    "        if len(links) >= num_articles:\n",
    "            break\n",
    "    \n",
    "    return links[:num_articles]\n",
    "\n",
    "# Function to scrape an article\n",
    "def scrape_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    return {\n",
    "        'article_name': article.title,\n",
    "        'topic': '',  # You might need to extract this separately if available\n",
    "        'source': url,\n",
    "        'date': article.publish_date,\n",
    "        'text': article.text\n",
    "    }\n",
    "\n",
    "# Main function to scrape articles and save to CSV\n",
    "def scrape_and_save_articles(base_url, num_articles, output_csv):\n",
    "    links = get_article_links(base_url, num_articles)\n",
    "    articles = []\n",
    "    \n",
    "    for link in links:\n",
    "        try:\n",
    "            article = scrape_article(link)\n",
    "            articles.append(article)\n",
    "            time.sleep(1)  # Respectful scraping: wait between requests\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {link}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Define base URL and number of articles to scrape\n",
    "BASE_URL = 'https://www.zeit.de/'\n",
    "NUM_ARTICLES = 100\n",
    "OUTPUT_CSV = 'zeit_articles.csv'\n",
    "\n",
    "# Run the scraper\n",
    "scrape_and_save_articles(BASE_URL, NUM_ARTICLES, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LandingPageArticle:\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'LandingPageArticle(title={self.title}, url={self.url})'\n",
    "    \n",
    "def get_article_links(zeit_online_url):\n",
    "\n",
    "response =requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.zeit.de/', headers=headers)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Can't get todays articles\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "articles = []\n",
    "for article_tag in soup.find_all('article'):\n",
    "    if 'zone-teaser--standard' not in article_tag.get('class', []):\n",
    "        continue\n",
    "    \n",
    "    if 'zplus' in article_tag.get('data-zplus', []):\n",
    "        continue\n",
    "    a_tag = article_tag.find('a', href=True)\n",
    "    if not a_tag:\n",
    "        continue\n",
    "    name = a_tag.get_text(strip=True)\n",
    "    url = a_tag['href']\n",
    "    # Assuming the date is stored in a <time> tag within the article\n",
    "    time_tag = article_tag.find('time')\n",
    "    date = time_tag['datetime'] if time_tag else 'Unknown'\n",
    "    if 'data-zplus' not in article_tag.attrs:\n",
    "        articles.append(Article(name=name, date=date, url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles : list[Article] = []\n",
    "for article_tag in soup.find_all('article'):\n",
    "    if 'zon-teaser--standard' not in article_tag.get('class', []) or \\\n",
    "        not article_tag.get('data-zplus') or 'zplus' == article_tag.get('data-zplus'):\n",
    "        continue\n",
    "\n",
    "    a_tag = article_tag.find('a', href=True)\n",
    "    if not a_tag or not a_tag.get('href').startswith('https://www.zeit.de/'):\n",
    "        continue\n",
    "    \n",
    "    title = a_tag.get_text(strip=True)\n",
    "    url = a_tag['href']\n",
    "    articles.append(Article(title=title, url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<newspaper.article.Article at 0x23a7b32d150>,\n",
       " <newspaper.article.Article at 0x23a77704e50>,\n",
       " <newspaper.article.Article at 0x23a7b31be90>,\n",
       " <newspaper.article.Article at 0x23a7a09e1d0>,\n",
       " <newspaper.article.Article at 0x23a7a07ff10>,\n",
       " <newspaper.article.Article at 0x23a7b338690>,\n",
       " <newspaper.article.Article at 0x23a7b338350>,\n",
       " <newspaper.article.Article at 0x23a7b354e50>,\n",
       " <newspaper.article.Article at 0x23a7a582050>,\n",
       " <newspaper.article.Article at 0x23a7b33b190>,\n",
       " <newspaper.article.Article at 0x23a7b2de010>,\n",
       " <newspaper.article.Article at 0x23a7b2dccd0>,\n",
       " <newspaper.article.Article at 0x23a7b2decd0>,\n",
       " <newspaper.article.Article at 0x23a794edf90>,\n",
       " <newspaper.article.Article at 0x23a482705d0>,\n",
       " <newspaper.article.Article at 0x23a7b364b10>,\n",
       " <newspaper.article.Article at 0x23a7b367650>,\n",
       " <newspaper.article.Article at 0x23a7b367d90>,\n",
       " <newspaper.article.Article at 0x23a7b365910>,\n",
       " <newspaper.article.Article at 0x23a7b36eb50>,\n",
       " <newspaper.article.Article at 0x23a7b36ec10>,\n",
       " <newspaper.article.Article at 0x23a7b36cc90>,\n",
       " <newspaper.article.Article at 0x23a7b36da50>,\n",
       " <newspaper.article.Article at 0x23a7b36ec90>,\n",
       " <newspaper.article.Article at 0x23a792e95d0>,\n",
       " <newspaper.article.Article at 0x23a7b39cd50>,\n",
       " <newspaper.article.Article at 0x23a7b39e350>,\n",
       " <newspaper.article.Article at 0x23a7b373990>,\n",
       " <newspaper.article.Article at 0x23a7b39b0d0>,\n",
       " <newspaper.article.Article at 0x23a7b399310>,\n",
       " <newspaper.article.Article at 0x23a7b39a450>,\n",
       " <newspaper.article.Article at 0x23a7b39ac10>,\n",
       " <newspaper.article.Article at 0x23a7b39bf50>,\n",
       " <newspaper.article.Article at 0x23a7b34f990>,\n",
       " <newspaper.article.Article at 0x23a7b3b19d0>,\n",
       " <newspaper.article.Article at 0x23a7b3b0f10>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = newspaper.article(\"https://www.zeit.de/gesellschaft/2024-06/gil-ofarim-antisemitismus-hotel-lepizig-verbrechen-podcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(article.tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
